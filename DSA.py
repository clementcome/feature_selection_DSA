import numpy as np
import vertica_python
from operator import itemgetter
import pandas as pd
import re
import time


def get_dataset(file_name, use_default_path=True):
    base_url = '../datasets/'
    if use_default_path:
        file = pd.read_csv(base_url+file_name+'.csv', sep=',')
    else:
        file = pd.read_csv(file_name+'.csv', sep=',')
    # file = file.replace("'", "''")
    file = file.apply(lambda x: x.astype(str).str.lower())
    return file


def get_dataset_with_separator(file_name, separ=','):
    base_url = '../datasets/'
    file = pd.read_csv(base_url+file_name+'.csv', sep=separ)
    # print(file)
    # file = file.replace("'", "''")
    file = file.apply(lambda x: x.astype(str).str.lower())
    return file


def get_cleaned_text(text):
    if text is None:
        return ''
    stopwords = ['a', 'the', 'of', 'on', 'in', 'an', 'and', 'is', 'at', 'are', 'as', 'be', 'but', 'by', 'for', 'it', 'no',
                 'not', 'or', 'such', 'that', 'their', 'there', 'these', 'to', 'was', 'with', 'they', 'will',  'v', 've', 'd']  # , 's']

    cleaned = re.sub(
        '[\W_]+', ' ', text.encode('ascii', 'ignore').decode('ascii'))
    # feature_one = re.sub(' +', '', cleaned).strip()
    feature_one = re.sub(' +', ' ', cleaned).strip()
    # feature_one = feature_one.replace(" s ", "''s  ")

    for x in stopwords:
        feature_one = feature_one.replace(' {} '.format(x), ' ')
        if feature_one.startswith('{} '.format(x)):
            feature_one = feature_one[len('{} '.format(x)):]
        if feature_one.endswith(' {}'.format(x)):
            feature_one = feature_one[:-len(' {}'.format(x))]
    return feature_one


def get_overlappings(k, file_path, query_column):
    conn_info = {'host': '127.0.0.1',
                 'port': 5433,
                 'user': 'olib92',
                 'password': 'Transformer2',
                 'database': 'xformer',
                 # autogenerated session label by default,
                 'session_label': 'some_label',
                 # 10 minutes timeout on queries
                 'read_timeout': 6000,
                 # default throw error on invalid UTF-8 results
                 'unicode_error': 'strict',
                 # SSL is disabled by default
                 'ssl': False,
                 # using server-side prepared statements is disabled by default
                 'use_prepared_statements': False,
                 # connection timeout is not enabled by default
                 # 'connection_timeout': 5
                 }
    connection = vertica_python.connect(**conn_info)
    cur = connection.cursor()
    data = get_dataset(file_path)[[query_column]]
    data[query_column] = data[query_column].apply(get_cleaned_text)
    distinct_clean_values = data[query_column].unique()
    joint_distinct_values = '\',\''.join(
        distinct_clean_values).encode('utf-8')
    # query = 'SELECT SUBQ.ids FROM (SELECT CONCAT(colid, CONCAT(\'_\',tableid)) AS ids,' \
    #         'CONCAT(colid, CONCAT(\'_\',CONCAT(tableid,CONCAT(\'_\',REGEXP_REPLACE(REGEXP_REPLACE(' \
    #         'tokenized, \'\W+\', \' \'), \' +\', \' \'))))) AS COL_ELEM from main_tokenized_union WHERE REGEXP_REPLACE(' \
    #         'REGEXP_REPLACE(tokenized, \'\W+\', \' \'), \' +\', \' \') IN (\'{}\') ' \
    #         'GROUP BY CONCAT(colid, CONCAT(\'_\',tableid)),CONCAT(colid, CONCAT(\'_\',CONCAT(tableid,CONCAT(\'_\',' \
    #         'REGEXP_REPLACE(REGEXP_REPLACE(tokenized, \'\W+\', \' \'), \' +\', \' \'))))) ) AS SUBQ GROUP BY SUBQ.ids ' \
    #         'HAVING COUNT(COL_ELEM) > {} ' \
    #         'ORDER BY COUNT(COL_ELEM) DESC LIMIT {};'.format(joint_distinct_values, 3, k)

    query = 'SELECT SUBQ.ids FROM (SELECT table_col_id AS ids,' \
            'CONCAT(table_col_id,CONCAT(\'_\',REGEXP_REPLACE(REGEXP_REPLACE(' \
            'tokenized, \'\W+\', \' \'), \' +\', \' \'))) AS COL_ELEM from cbi_inverted_index_2 WHERE REGEXP_REPLACE(' \
            'REGEXP_REPLACE(tokenized, \'\W+\', \' \'), \' +\', \' \') IN (\'{}\') ' \
            'GROUP BY table_col_id,CONCAT(table_col_id,CONCAT(\'_\',' \
            'REGEXP_REPLACE(REGEXP_REPLACE(tokenized, \'\W+\', \' \'), \' +\', \' \'))) ) AS SUBQ GROUP BY SUBQ.ids ' \
            'HAVING COUNT(COL_ELEM) > {} ' \
            'ORDER BY COUNT(COL_ELEM) DESC LIMIT {};'.format(
                joint_distinct_values, 3, k)
    a = time.time()
    cur.execute(query)
    result = [item for sublist in cur.fetchall() for item in sublist]
    t = time.time()-a
    return result, t


def enrich_data_all_candidates(dataset_name, data_path, query_column, target_column, k, universal_k):
    db_access_total_time = 0
    join_total_time = 0
    column_added_count = 0
    total_save_time = 0

    conn_info = {'host': '127.0.0.1',
                 'port': 5433,
                 'user': 'olib92',
                 'password': 'Transformer2',
                 'database': 'xformer',
                 # autogenerated session label by default,
                 'session_label': 'some_label',
                 # 10 minutes timeout on queries
                 'read_timeout': 6000,
                 # default throw error on invalid UTF-8 results
                 'unicode_error': 'strict',
                 # SSL is disabled by default
                 'ssl': False,
                 # using server-side prepared statements is disabled by default
                 'use_prepared_statements': False,
                 # connection timeout is not enabled by default
                 # 'connection_timeout': 5
                 }
    connection = vertica_python.connect(**conn_info)
    cur = connection.cursor()

    table_ids = []
    column_ids = []

    # with open("../{}_final_col_ids_list_1000.txt".format(dataset_name), "r") as f:
    #     for line in f:
    #             table_ids.append(int(line.split('_')[1].strip()))
    #             column_ids.append(int(line.split('_')[0].strip()))

    overlappings, overlapping_finding_time = get_overlappings(
        universal_k, dataset_name, query_column)
    # overlappings = []
    overlapping_finding_time = 0
    #
    # with open("../cache/{}_{}_final_col_ids_list_all.txt".format(dataset_name, universal_k), "w") as f:
    #     for s in overlappings:
    #         f.write(str(s) + "\n")

    # with open("../cache/{}_{}_final_col_ids_list_all.txt".format(dataset_name, universal_k), "r") as f:
    #     for line in f:
    #         overlappings+=[line.strip()]

    a = time.time()
    for o in overlappings:
        table_ids.append(int(o.split('_')[0].strip()))
        column_ids.append(int(o.split('_')[1].strip()))

    s = [str(i) for i in table_ids]
    cur.execute('SELECT tableid, maxcol from cbi_inverted_index_2 WHERE tableid IN (\'{}\');'.format(
        '\',\''.join(s)))
    result = pd.DataFrame(cur.fetchall(), columns=[
                          'tableid', 'max_col_id'])
    max_column_dict = result.set_index(
        'tableid').to_dict()['max_col_id']
    data = get_dataset(data_path, use_default_path=False)[
        [query_column] + [target_column]]
    data[query_column] = data[query_column].apply(
        lambda x: get_cleaned_text(x))

    data[target_column] = data[target_column].astype('float')
    input_size = len(data)

    column_name = []
    column_content = []

    final_tables = []
    final_cols = []
    final_overlapping_cols = []
    tables_fetch_query = 'SELECT tableid, colid, rowid, table_row_id, tokenized FROM cbi_inverted_index_2 WHERE tableid IN (\'{}\') order by tableid, colid, rowid;'.format(
        '\',\''.join(s))
    cur.execute(tables_fetch_query)
    external_tables = pd.DataFrame(cur.fetchall(), columns=[
                                   'tableid', 'colid', 'rowid', 'table_row_id', 'tokenized'])

    temp = external_tables.sort_values(by=['tableid', 'rowid', 'colid']).groupby(
        ['tableid', 'rowid']).tokenized.apply(list).reset_index()

    # numerics_dict = {}
    # with open("../{}_floats_{}_{}.txt".format(dataset_name, universal_k, input_size), "r") as f:
    #     for line in f:
    #         key = '{}_{}'.format(line.split('_')[0].strip(), line.split('_')[1].strip())
    #         if key in numerics_dict:
    #             numerics_dict[key] = numerics_dict[key] + [int(line.split('_')[2].strip())]
    #         else:
    #             numerics_dict[key] = [int(line.split('_')[2].strip())]

    data_prepration_time = time.time() - a
    start_time = time.time()

    table_counter = 1
    table_number = len(table_ids)
    for i in np.arange(table_number):
        # print('Table {}/{}'.format(table_counter, table_number))
        table_counter += 1
        column = column_ids[i]
        table = table_ids[i]
        # max_col = max_column[i]
        max_col = max_column_dict[table]

        a = time.time()
        temp_condition = temp['tableid'] == table
        external_temp = temp[temp_condition].drop(['tableid'], axis=1)
        temp_external_join_table = external_temp.set_index('rowid').to_dict()[
            'tokenized']
        t = time.time() - a
        db_access_total_time += t

        df_temp = pd.DataFrame.from_dict(
            temp_external_join_table, orient='index').drop_duplicates(column, keep='last')

        a = time.time()
        df_cd = pd.merge(data, df_temp, how='left',
                         left_on=query_column, right_on=column)
        t = time.time() - a
        join_total_time += t

        a = time.time()
        df_cd.applymap(lambda x: str(x).replace(',', '').replace('$', '').replace('.', '').replace(' ', '').replace(':', '').replace(';', '').replace(
            '%', '').replace('&', '').replace('?', '').replace('/', '')).to_csv('temp_{}_{}.csv'.format(dataset_name, universal_k), index=False)
        df_cd = pd.read_csv('temp_{}_{}.csv'.format(
            dataset_name, universal_k))

        t = time.time() - a
        total_save_time += t

        for c in range(max_col + 1):
            # if '{}_{}'.format(table, column) not in numerics_dict or c not in numerics_dict['{}_{}'.format(table, column)]:
            #     continue
            if c != column:
                column_to_be_added = df_cd[str(c)]
                column_added_count += 1
                column_name += [str(table) + '_' + str(c)]
                column_content += [column_to_be_added.copy()]
                # final_cols += [str(c)]
                # final_overlapping_cols += [str(column)]
                # final_tables += [str(table)]
    # sorted_list = [x for _, x in sorted(zip(column_correlation, column_name))]
    # sorted_list = [[y, x] for _, y, x in sorted(zip(column_correlation, column_name, column_content))]
    # for important_column in sorted_list[:k]:
    #     data[important_column[0]] = important_column[1]

    overall_list = []
    for i in np.arange(len(column_name)):
        overall_list += [[column_name[i], column_content[i]]]
    for important_column_index in np.arange(len(overall_list)):
        important_column = overall_list[important_column_index]
        data['{}_{}'.format(
            important_column[0], important_column_index)] = important_column[1]
    data.to_csv('../enriched/all_webtables_{}_{}.csv'.format(
        dataset_name, universal_k), index=False)

    connection.close()


def enrich_data(dataset_name, data_path, query_column, target_column, k, universal_k):
    ignored_tables = 0
    db_access_total_time = 0
    join_total_time = 0
    column_added_count = 0
    total_save_time = 0

    rule_total_time = 0

    conn_info = {'host': '127.0.0.1',
                 'port': 5433,
                 'user': 'olib92',
                 'password': 'Transformer2',
                 'database': 'xformer',
                 # autogenerated session label by default,
                 'session_label': 'some_label',
                 # 10 minutes timeout on queries
                 'read_timeout': 6000,
                 # default throw error on invalid UTF-8 results
                 'unicode_error': 'strict',
                 # SSL is disabled by default
                 'ssl': False,
                 # using server-side prepared statements is disabled by default
                 'use_prepared_statements': False,
                 # connection timeout is not enabled by default
                 # 'connection_timeout': 5
                 }
    connection = vertica_python.connect(**conn_info)
    cur = connection.cursor()

    table_ids = []
    column_ids = []

    # with open("../{}_final_col_ids_list_1000.txt".format(dataset_name), "r") as f:
    #     for line in f:
    #             table_ids.append(int(line.split('_')[1].strip()))
    #             column_ids.append(int(line.split('_')[0].strip()))

    overlappings, overlapping_finding_time = get_overlappings(
        universal_k, dataset_name, query_column)
    # overlappings = []
    overlapping_finding_time = 0
    #
    # with open("../cache/{}_{}_final_col_ids_list_all.txt".format(dataset_name, universal_k), "w") as f:
    #     for s in overlappings:
    #         f.write(str(s) + "\n")
    #
    # with open("../cache/{}_{}_final_col_ids_list_all.txt".format(dataset_name, universal_k), "r") as f:
    #     for line in f:
    #         overlappings+=[line.strip()]

    a = time.time()
    for o in overlappings:
        table_ids.append(int(o.split('_')[0].strip()))
        column_ids.append(int(o.split('_')[1].strip()))

    s = [str(i) for i in table_ids]
    cur.execute('SELECT tableid, maxcol from cbi_inverted_index_2 WHERE tableid IN (\'{}\');'.format(
        '\',\''.join(s)))
    result = pd.DataFrame(cur.fetchall(), columns=[
                          'tableid', 'max_col_id'])
    max_column_dict = result.set_index(
        'tableid').to_dict()['max_col_id']
    data = get_dataset(data_path, use_default_path=False)[
        [query_column] + [target_column]]
    data[query_column] = data[query_column].apply(
        lambda x: get_cleaned_text(x))

    data[target_column] = data[target_column].astype('float')
    input_size = len(data)

    column_name = []
    column_content = []

    final_tables = []
    final_cols = []
    final_overlapping_cols = []
    tables_fetch_query = 'SELECT tableid, colid, rowid, table_row_id, tokenized FROM cbi_inverted_index_2 WHERE tableid IN (\'{}\') order by tableid, colid, rowid;'.format(
        '\',\''.join(s))
    cur.execute(tables_fetch_query)
    external_tables = pd.DataFrame(cur.fetchall(), columns=[
                                   'tableid', 'colid', 'rowid', 'table_row_id', 'tokenized'])

    temp = external_tables.sort_values(by=['tableid', 'rowid', 'colid']).groupby(
        ['tableid', 'rowid']).tokenized.apply(list).reset_index()

    numerics_dict = {}
    with open("../{}_floats_{}_{}.txt".format(dataset_name, universal_k, input_size), "r") as f:
        for line in f:
            key = '{}_{}'.format(line.split(
                '_')[0].strip(), line.split('_')[1].strip())
            if key in numerics_dict:
                numerics_dict[key] = numerics_dict[key] + \
                    [int(line.split('_')[2].strip())]
            else:
                numerics_dict[key] = [int(line.split('_')[2].strip())]

    data_prepration_time = time.time() - a
    start_time = time.time()

    table_counter = 1
    table_number = len(table_ids)
    for i in np.arange(table_number):
        # print('Table {}/{}'.format(table_counter, table_number))
        table_counter += 1
        column = column_ids[i]
        table = table_ids[i]
        # max_col = max_column[i]
        max_col = max_column_dict[table]

        a = time.time()
        temp_condition = temp['tableid'] == table
        external_temp = temp[temp_condition].drop(['tableid'], axis=1)
        temp_external_join_table = external_temp.set_index('rowid').to_dict()[
            'tokenized']
        t = time.time() - a
        db_access_total_time += t

        df_temp = pd.DataFrame.from_dict(
            temp_external_join_table, orient='index').drop_duplicates(column, keep='last')

        a = time.time()
        df_cd = pd.merge(data, df_temp, how='left',
                         left_on=query_column, right_on=column)
        t = time.time() - a
        join_total_time += t

        a = time.time()
        df_cd.applymap(lambda x: str(x).replace(',', '').replace('$', '').replace('.', '').replace(' ', '').replace(':', '').replace(';', '').replace(
            '%', '').replace('&', '').replace('?', '').replace('/', '')).to_csv('temp_{}_{}.csv'.format(dataset_name, universal_k), index=False)
        df_cd = pd.read_csv('temp_{}_{}.csv'.format(
            dataset_name, universal_k))

        t = time.time() - a
        total_save_time += t

        for c in range(max_col + 1):
            # if '{}_{}'.format(table, column) not in numerics_dict or c not in numerics_dict['{}_{}'.format(table, column)]:
            #     continue
            if c != column:
                column_to_be_added = df_cd[str(c)]
                column_added_count += 1
                column_name += [str(table) + '_' + str(c)]
                column_content += [column_to_be_added.copy()]
                final_cols += [str(c)]
                final_overlapping_cols += [str(column)]
                final_tables += [str(table)]
    # sorted_list = [x for _, x in sorted(zip(column_correlation, column_name))]
    # sorted_list = [[y, x] for _, y, x in sorted(zip(column_correlation, column_name, column_content))]
    # for important_column in sorted_list[:k]:
    #     data[important_column[0]] = important_column[1]

    overall_list = []
    for i in np.arange(len(column_name)):
        overall_list += [[column_name[i], column_content[i]]]
    for important_column_index in np.arange(len(overall_list)):
        important_column = overall_list[important_column_index]
        data['{}_{}'.format(
            important_column[0], important_column_index)] = important_column[1]
    data.to_csv('../enriched/to_join_or_not_to_join_{}_{}.csv'.format(
        dataset_name, universal_k), index=False)

    with open("to_join_or_not_runtime_experiments_2.txt", "a+") as f:
        f.write(
            '**********************{}***********{}*************'.format(dataset_name, universal_k))
        f.write('total rule calculation time: {}, average rule time: {}, total_number_of_tables: {}'.format(
            rule_total_time, rule_total_time / table_number, table_number) + "\n")
        f.write('total join time: {}, average join time: {}, total number of tables: {}'.format(
            join_total_time, join_total_time / table_number, table_number) + "\n")
        f.write('ignored tables: {}'.format(ignored_tables) + "\n")

    connection.close()


enrich_data_all_candidates(
    'universities', '../datasets/universities', 'name', 'target', 100, 50)
enrich_data_all_candidates(
    'universities', '../datasets/universities', 'name', 'target', 100, 100)
enrich_data_all_candidates(
    'universities', '../datasets/universities', 'name', 'target', 100, 1000)
enrich_data_all_candidates(
    'universities', '../datasets/universities', 'name', 'target', 100, 5000)
enrich_data_all_candidates(
    'universities', '../datasets/universities', 'name', 'target', 100, 10000)

enrich_data_all_candidates('presidential_final_3000',
                           '../datasets/presidential_final_3000', 'County', 'Votes', 100, 50)
enrich_data_all_candidates('presidential_final_3000',
                           '../datasets/presidential_final_3000', 'County', 'Votes', 100, 100)
enrich_data_all_candidates('presidential_final_3000',
                           '../datasets/presidential_final_3000', 'County', 'Votes', 100, 1000)
enrich_data_all_candidates('presidential_final_3000',
                           '../datasets/presidential_final_3000', 'County', 'Votes', 100, 5000)
enrich_data_all_candidates('presidential_final_3000',
                           '../datasets/presidential_final_3000', 'County', 'Votes', 100, 10000)

enrich_data_all_candidates(
    'movie', '../datasets/movie', 'movie_title', 'imdb_score', 100, 50)
enrich_data_all_candidates(
    'movie', '../datasets/movie', 'movie_title', 'imdb_score', 100, 100)
enrich_data_all_candidates(
    'movie', '../datasets/movie', 'movie_title', 'imdb_score', 100, 1000)
enrich_data_all_candidates(
    'movie', '../datasets/movie', 'movie_title', 'imdb_score', 100, 5000)
enrich_data_all_candidates(
    'movie', '../datasets/movie', 'movie_title', 'imdb_score', 100, 10000)

enrich_data_all_candidates(
    'pageviews_final_11000', '../datasets/pageviews_final_11000', 'name', 'visit', 100, 50)
enrich_data_all_candidates(
    'pageviews_final_11000', '../datasets/pageviews_final_11000', 'name', 'visit', 100, 100)
enrich_data_all_candidates(
    'pageviews_final_11000', '../datasets/pageviews_final_11000', 'name', 'visit', 100, 1000)
enrich_data_all_candidates(
    'pageviews_final_11000', '../datasets/pageviews_final_11000', 'name', 'visit', 100, 5000)
enrich_data_all_candidates(
    'pageviews_final_11000', '../datasets/pageviews_final_11000', 'name', 'visit', 100, 10000)
#
enrich_data_all_candidates('worldcitiespop_top_40000',
                           '../datasets/worldcitiespop_top_40000', 'City', 'Population', 100, 50)
enrich_data_all_candidates('worldcitiespop_top_40000',
                           '../datasets/worldcitiespop_top_40000', 'City', 'Population', 100, 100)
enrich_data_all_candidates('worldcitiespop_top_40000',
                           '../datasets/worldcitiespop_top_40000', 'City', 'Population', 100, 1000)
enrich_data_all_candidates('worldcitiespop_top_40000',
                           '../datasets/worldcitiespop_top_40000', 'City', 'Population', 100, 5000)
enrich_data_all_candidates('worldcitiespop_top_40000',
                           '../datasets/worldcitiespop_top_40000', 'City', 'Population', 100, 10000)
