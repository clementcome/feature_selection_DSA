from typing import Tuple, List, Dict
from timer.timer import timer

import numpy as np
import pandas as pd
import vertica_python
import re

DBConnection = vertica_python.vertica.connection.Connection


@timer
def get_dataset(file_name: str, use_default_path: bool = True, separ: str = ",") -> pd.DataFrame:
    """
    Gets the dataset located at file_name
    If use_default_path then it adds a base_url before the file name
    Converts all the data into string data

    Parameters
    ----------
    file_name : str
        Name of the file to retrieve the dataset from
    use_default_path : bool, optional
        Boolean indicating whether we will be using "../datasets/" as a base_url or not, by default True
    separ : str, optional
        Separator for importing the dataset, by default ","

    Returns
    -------
    pd.DataFrame
        Dataset contained in the file designated by file_name
    """
    base_url = '../datasets/'
    if use_default_path:
        file = pd.read_csv(base_url+file_name+'.csv', sep=separ)
    else:
        file = pd.read_csv(file_name+'.csv', sep=separ)
    # file = file.replace("'", "''")
    file = file.apply(lambda x: x.astype(str).str.lower())
    return file


@timer
def get_cleaned_text(text: str) -> str:
    """Removes non-word characters and stopwords."""
    if text is None:
        return ''
    stopwords = ['a', 'the', 'of', 'on', 'in', 'an', 'and', 'is', 'at', 'are', 'as', 'be', 'but', 'by', 'for', 'it', 'no',
                 'not', 'or', 'such', 'that', 'their', 'there', 'these', 'to', 'was', 'with', 'they', 'will',  'v', 've', 'd']  # , 's']

    cleaned = re.sub(
        '[\W_]+', ' ', text.encode('ascii', 'ignore').decode('ascii'))
    # feature_one = re.sub(' +', '', cleaned).strip()
    feature_one = re.sub(' +', ' ', cleaned).strip()
    # feature_one = feature_one.replace(" s ", "''s  ")

    for x in stopwords:
        feature_one = feature_one.replace(' {} '.format(x), ' ')
        if feature_one.startswith('{} '.format(x)):
            feature_one = feature_one[len('{} '.format(x)):]
        if feature_one.endswith(' {}'.format(x)):
            feature_one = feature_one[:-len(' {}'.format(x))]
    return feature_one


@timer
def connect_to_database() -> DBConnection:
    """
    Example
    -------
    Standard use is :\n
    connection = connect_to_database()\n
    cur = connection.cursor()\n
    cur.execute(query1)\n
    cur.fetchall()\n
    cur.execute(query1)\n
    ...\n
    connection.close()\n

    Returns
    -------
    vertica_python.vertica.connnection.Connection
        connection from vertica_python
    """
    conn_info = {
        'host': '127.0.0.1',
        'port': 5433,
        'user': 'olib92',
        'password': 'Transformer2',
        'database': 'xformer',
        # autogenerated session label by default,
        'session_label': 'some_label',
        # 10 minutes timeout on queries
        'read_timeout': 6000,
        # default throw error on invalid UTF-8 results
        'unicode_error': 'strict',
        # SSL is disabled by default
        'ssl': False,
        # using server-side prepared statements is disabled by default
        'use_prepared_statements': False,
        # connection timeout is not enabled by default
        # 'connection_timeout': 5
    }
    connection = vertica_python.connect(**conn_info)
    return connection


@timer
def get_overlappings(k: int, data_path: str, query_column: str, connection: DBConnection = None) -> List[str]:
    """
    Choose k table to perform join with the base dataset.
    The order of the tables is determined by the number of appearances of every value
    in the query column of the base dataset.

    Parameters
    ----------
    k : int
        Max number of table to join the base dataset to
    data_path : str
        Path to the dataset
    query_column : str
        Column of the base dataset to use as join key

    Returns
    -------
    List[str]
        List of "tableid_colid" where\n
        - tableid is the table to join\n
        - colid is the name of the column to use as join key
    """

    if connection == None:
        connection = connect_to_database()
    cur = connection.cursor()

    data = get_dataset(data_path)[[query_column]]
    data[query_column] = data[query_column].apply(get_cleaned_text)

    distinct_clean_values = data[query_column].unique()
    joint_distinct_values = '\',\''.join(
        distinct_clean_values).encode('utf-8')

    query = 'SELECT SUBQ.ids FROM (SELECT table_col_id AS ids,' \
            'CONCAT(table_col_id,CONCAT(\'_\',REGEXP_REPLACE(REGEXP_REPLACE(' \
            'tokenized, \'\W+\', \' \'), \' +\', \' \'))) AS COL_ELEM from cbi_inverted_index_2 WHERE REGEXP_REPLACE(' \
            'REGEXP_REPLACE(tokenized, \'\W+\', \' \'), \' +\', \' \') IN (\'{}\') ' \
            'GROUP BY table_col_id,CONCAT(table_col_id,CONCAT(\'_\',' \
            'REGEXP_REPLACE(REGEXP_REPLACE(tokenized, \'\W+\', \' \'), \' +\', \' \'))) ) AS SUBQ GROUP BY SUBQ.ids ' \
            'HAVING COUNT(COL_ELEM) > {} ' \
            'ORDER BY COUNT(COL_ELEM) DESC LIMIT {};'.format(
                joint_distinct_values, 3, k)
    cur.execute(query)

    result = [item for sublist in cur.fetchall() for item in sublist]
    connection.close()
    return result


def extract_table_and_col_id(overlappings: List[str]) -> Tuple[List[int], List[int]]:
    """
    Return a list of table_id and a list of column_id
    extracted from the result of overlappings.
    """
    table_id_list = []
    column_id_list = []
    for o in overlappings:
        table_id_list.append(int(o.split('_')[0].strip()))
        column_id_list.append(int(o.split('_')[1].strip()))
    return table_id_list, column_id_list


@timer
def table_max_column(table_id_list: List[int], connection: DBConnection = None) -> Dict[int, int]:

    if connection == None:
        connection = connect_to_database()
    cur = connection.cursor()

    # Transforming table_ids from integers to strings
    s = [str(i) for i in table_id_list]
    cur.execute('SELECT tableid, maxcol from cbi_inverted_index_2 WHERE tableid IN (\'{}\');'.format(
        '\',\''.join(s)))
    result = pd.DataFrame(cur.fetchall(), columns=[
                          'tableid', 'max_col_id'])
    max_column_dict = result.set_index(
        'tableid').to_dict()['max_col_id']
    return max_column_dict


def get_clean_dataset(data_path: str, query_column: str, target_column: str) -> pd.DataFrame:
    data = get_dataset(data_path, use_default_path=False)[
        [query_column, target_column]]
    data[query_column] = data[query_column].apply(get_cleaned_text)
    # Only supports regression until now
    data[target_column] = data[target_column].astype(float)
    return data


@timer
def get_external_tables(table_id_list: List[int], connection: DBConnection = None) -> pd.DataFrame:

    if connection == None:
        connection = connect_to_database()
    cur = connection.cursor()

    s = [str(table_id) for table_id in table_id_list]
    tables_fetch_query = 'SELECT tableid, colid, rowid, table_row_id, tokenized FROM cbi_inverted_index_2 WHERE tableid IN (\'{}\') order by tableid, colid, rowid;'.format(
        '\',\''.join(s))
    cur.execute(tables_fetch_query)
    external_tables = pd.DataFrame(cur.fetchall(), columns=[
                                   'tableid', 'colid', 'rowid', 'table_row_id', 'tokenized'])

    temp = external_tables.sort_values(by=['tableid', 'rowid', 'colid']).groupby(
        ['tableid', 'rowid']).tokenized.apply(list).reset_index()

    return temp
